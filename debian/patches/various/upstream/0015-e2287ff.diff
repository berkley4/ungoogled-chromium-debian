From e2287ff27b62a31abc06cc448b1427a9233cdc36 Mon Sep 17 00:00:00 2001
From: Yi Chou <yich@google.com>
Date: Thu, 11 Apr 2024 17:19:44 +0000
Subject: [PATCH] on_device_model: Limit the max token size for input

We should limit the max token size of the input even if the client
doesn't provide it properly in the input parameters, otherwise the
engine will crash, and that may cause the browser to restart.

BUG=b:333686184
TEST=Provide a very long input without setting max_token, and no crash

Change-Id: Ideafe69ce4cc3a9490e20449db6aa7f6fabfdfa9
Reviewed-on: https://chromium-review.googlesource.com/c/chromium/src/+/5446747
Commit-Queue: Yi Chou <yich@google.com>
Reviewed-by: Clark DuVall <cduvall@chromium.org>
Cr-Commit-Position: refs/heads/main@{#1285929}
---

--- a/services/on_device_model/ml/on_device_model_executor.cc
+++ b/services/on_device_model/ml/on_device_model_executor.cc
@@ -4,6 +4,8 @@
 
 #include "services/on_device_model/ml/on_device_model_executor.h"
 
+#include <algorithm>
+#include <cstdint>
 #include <memory>
 #include <optional>
 #include <string>
@@ -36,6 +38,8 @@
 namespace ml {
 namespace {
 
+constexpr uint32_t kReserveTokensForSafety = 2;
+
 const base::FeatureParam<int> kMaxTopK{
     &optimization_guide::features::kOptimizationGuideOnDeviceModel,
     "on_device_model_max_topk", 128};
@@ -242,10 +246,12 @@
  public:
   SessionImpl(const ChromeML& chrome_ml,
               ChromeMLModel model,
+              uint32_t max_tokens,
               scoped_refptr<LanguageDetector> language_detector,
               std::optional<uint32_t> adaptation_id)
       : chrome_ml_(chrome_ml),
         model_(model),
+        max_tokens_(max_tokens),
         language_detector_(std::move(language_detector)),
         adaptation_id_(adaptation_id) {}
   ~SessionImpl() override = default;
@@ -265,7 +271,8 @@
     ChromeMLExecuteOptions options{
         .prompt = input->text.c_str(),
         .context_mode = GetContextMode(*input) | ContextMode::kSave,
-        .max_tokens = input->max_tokens.value_or(0),
+        .max_tokens =
+            std::min(input->max_tokens.value_or(max_tokens_), max_tokens_),
         .token_offset = input->token_offset.value_or(0),
         .context_saved_fn = &context_saved_fn,
         .top_k = GetTopK(input->top_k),
@@ -296,7 +303,8 @@
     ChromeMLExecuteOptions options{
         .prompt = input->text.c_str(),
         .context_mode = GetContextMode(*input),
-        .max_tokens = input->max_tokens.value_or(0),
+        .max_tokens =
+            std::min(input->max_tokens.value_or(max_tokens_), max_tokens_),
         .token_offset = input->token_offset.value_or(0),
         .max_output_tokens = input->max_output_tokens.value_or(0),
         .score_ts_interval = ts_interval,
@@ -329,6 +337,7 @@
   bool clear_context_ = true;
   const raw_ref<const ChromeML> chrome_ml_;
   ChromeMLModel model_;
+  const uint32_t max_tokens_;
   const scoped_refptr<LanguageDetector> language_detector_;
   std::unique_ptr<Responder> responder_;
   std::set<std::unique_ptr<ContextHolder>> context_holders_;
@@ -367,8 +376,9 @@
 
 std::unique_ptr<on_device_model::OnDeviceModel::Session>
 OnDeviceModelExecutor::CreateSession(std::optional<uint32_t> adaptation_id) {
-  return std::make_unique<SessionImpl>(*chrome_ml_, model_, language_detector_,
-                                       adaptation_id);
+  return std::make_unique<SessionImpl>(*chrome_ml_, model_,
+                                       max_tokens_ - kReserveTokensForSafety,
+                                       language_detector_, adaptation_id);
 }
 
 DISABLE_CFI_DLSYM
@@ -441,6 +451,8 @@
     }
   }
 
+  max_tokens_ = std::max(params->max_tokens, kReserveTokensForSafety);
+
   auto model_proto_dispose =
       CreateWeakCallbackFn(&OnDeviceModelExecutor::DisposeModelProto, this);
   const ChromeMLModelData data = {
@@ -456,7 +468,7 @@
       .sentencepiece_model_proto_size = sentencepiece_model_proto_->length(),
       .sentencepiece_model_proto_dispose = &sentencepiece_model_proto_dispose,
       .model_data = &data,
-      .max_tokens = params->max_tokens,
+      .max_tokens = max_tokens_,
       .temperature = 0.0f,
       .top_k = kMaxTopK.Get(),
       .ts_dimension = params->ts_dimension.value_or(0),
--- a/services/on_device_model/ml/on_device_model_executor.h
+++ b/services/on_device_model/ml/on_device_model_executor.h
@@ -5,6 +5,7 @@
 #ifndef SERVICES_ON_DEVICE_MODEL_ML_ON_DEVICE_MODEL_EXECUTOR_H_
 #define SERVICES_ON_DEVICE_MODEL_ML_ON_DEVICE_MODEL_EXECUTOR_H_
 
+#include <cstdint>
 #include <functional>
 #include <memory>
 
@@ -70,6 +71,7 @@
 
   ChromeMLModel model_ = 0;
   scoped_refptr<base::SequencedTaskRunner> task_runner_;
+  uint32_t max_tokens_ = 0;
 };
 
 }  // namespace ml
